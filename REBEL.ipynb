{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nptmulKkC0_9"
      },
      "source": [
        "## REBEL: RErank BEyond reLevance\n",
        "This notebook will walk you through how to set up and run the REBEL method for RAG reranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install llama_index from our fork, once the PR is merged this will just be a normal llama_index install\n",
        "# NOTE, restart the session after installing then skip this cell\n",
        "!git clone https://github.com/bvarjavand/llama_index.git\n",
        "!cd llama_index && pip install --quiet -e .\n",
        "!cd llama_index/llama-index-core && pip install --quiet -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GIsXBynID4Kq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "OPENAI_API_KEY=\"sk-\" # put your openai key here\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "OPENAI_API_BASE=\"\" # optional, if you want to use another hosted endpoint\n",
        "if OPENAI_API_BASE:\n",
        "    os.environ[\"OPENAI_API_BASE\"] = OPENAI_API_BASE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EZBbcVK8_o8r"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core.postprocessor import LLMRerank\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from IPython.display import Markdown, display\n",
        "from llama_index.core import Settings\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "if OPENAI_API_BASE:\n",
        "    Settings.llm = OpenAI(temperature=0, api_key=OPENAI_API_KEY, model=\"gpt-4o\", api_base=OPENAI_API_BASE)\n",
        "else:\n",
        "    Settings.llm = OpenAI(temperature=0, api_key=OPENAI_API_KEY, model=\"gpt-4o\")\n",
        "Settings.chunk_size = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVTvYfViEkp3"
      },
      "source": [
        "## Build RAG index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wid09ABpD09q"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "# save documents from wikipedia with these title(s):\n",
        "wiki_titles = [\n",
        "    \"Vincent van Gogh\",\n",
        "]\n",
        "\n",
        "# pull documents from wikipedia\n",
        "data_path = Path(\"data_wiki\")\n",
        "for title in wiki_titles:\n",
        "    response = requests.get(\n",
        "        \"https://en.wikipedia.org/w/api.php\",\n",
        "        params={\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"titles\": title,\n",
        "            \"prop\": \"extracts\",\n",
        "            \"explaintext\": True,\n",
        "        },\n",
        "    ).json()\n",
        "    page = next(iter(response[\"query\"][\"pages\"].values()))\n",
        "    wiki_text = page[\"extract\"]\n",
        "\n",
        "    if not data_path.exists():\n",
        "        Path.mkdir(data_path)\n",
        "\n",
        "    with open(data_path / f\"{title}.txt\", \"w\") as fp:\n",
        "        fp.write(wiki_text)\n",
        "\n",
        "if OPENAI_API_BASE:\n",
        "    embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\", api_key=OPENAI_API_KEY, api_base=OPENAI_API_BASE)\n",
        "else:\n",
        "    embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\", api_key=OPENAI_API_KEY)\n",
        "\n",
        "# load documents\n",
        "documents = SimpleDirectoryReader(\"./data_wiki/\").load_data()\n",
        "# build index\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, embed_model=embed_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cetRzJYKqGw"
      },
      "source": [
        "## Define query string, retrieve nodes, and rerank them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Lq70au8FG64z"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core import QueryBundle\n",
        "from llama_index.core.postprocessor import REBELRerank\n",
        "\n",
        "### You can define any query string you want here ###\n",
        "query_str = \"Which date did Paul Gauguin arrive in Arles?\"\n",
        "query_bundle = QueryBundle(query_str)\n",
        "\n",
        "# configure retriever\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=50,\n",
        ")\n",
        "\n",
        "# retrieve nodes\n",
        "retrieved_nodes = retriever.retrieve(query_bundle)\n",
        "\n",
        "# configure reranker\n",
        "if OPENAI_API_BASE:\n",
        "    reranked = REBELRerank(llm=OpenAI(model='gpt-4o', api_base=OPENAI_API_BASE), top_n=2)\n",
        "else:\n",
        "    reranked = REBELRerank(llm=OpenAI(model='gpt-4o'), top_n=2)\n",
        "\n",
        "# rerank nodes\n",
        "reranked_nodes = reranked.postprocess_nodes(retrieved_nodes, query_bundle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pUfXZdg7KRwA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def pretty_print(df):\n",
        "    return display(HTML(df.to_html().replace(\"\\\\n\", \"<br>\")))\n",
        "\n",
        "\n",
        "def visualize_nodes(nodes) -> None:\n",
        "    result_dicts = []\n",
        "    for node in nodes:\n",
        "        result_dict = {\"Score\": node.score, \"Text\": node.node.get_text()}\n",
        "        result_dicts.append(result_dict)\n",
        "\n",
        "    pretty_print(pd.DataFrame(result_dicts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCpsnkxhL4a9"
      },
      "source": [
        "### Top 3 nodes from initial retrieval:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "id": "ONNCmM61KYv-",
        "outputId": "0b11c27d-3fdc-4653-8b87-7c7b2a04ea30"
      },
      "outputs": [],
      "source": [
        "visualize_nodes(retrieved_nodes[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrRG-5tIL7XG"
      },
      "source": [
        "### Top 3 nodes from reranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "id": "xaZ3ybmRKalQ",
        "outputId": "39b2f410-bc7b-47c4-a53f-7bff672a87b9"
      },
      "outputs": [],
      "source": [
        "visualize_nodes(reranked_nodes[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TfHHbm2KeHG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
